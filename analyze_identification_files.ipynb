{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc44bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from handling_acc_files import HelperMethod\n",
    "from create_reference_from_tsv_and_pepxml import ReferenceWriter\n",
    "from collections import defaultdict\n",
    "from create_PSM_df import PSM_FDR\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from create_PSM_df import PSM_FDR\n",
    "from ReadAccTaxon import ReadAccTaxon\n",
    "from collections import defaultdict\n",
    "from handling_acc_files import HelperMethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18000bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb6ebd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accs_from_df(x_tandem_result_tsv, db_type, decoy_tag):\n",
    "    if db_type == 'uniprot' or db_type == 'swissprot':\n",
    "        accs = set()\n",
    "        for acc in pd.read_csv(str(x_tandem_result_tsv), delimiter='\\t')['Protein'].tolist():\n",
    "            try:\n",
    "                accs.add(acc.split()[0].split('|')[1])\n",
    "            # e.g. CRAP: KKA1_ECOLX\n",
    "            except IndexError:\n",
    "                accs.add(acc.split()[0])\n",
    "    # ncbi: 'generic|AC:5988671|WP_080217951.1', 'generic|AC:2500558_REVERSED|EQV03804.1 ERA83742.1 WP_021536075.1-REVERSED'\n",
    "    # problem: generic|AC:373747|pir||D85980 WP_000133047.1 AAG58304.1 -> maxsplit=2\n",
    "    # CRAP accs starts with 'sp|' or Index Error\n",
    "    elif db_type == 'ncbi':\n",
    "        accs = set()\n",
    "        for acc in pd.read_csv(str(x_tandem_result_tsv), delimiter='\\t')['Protein'].tolist():\n",
    "            try:\n",
    "                if decoy_tag not in acc and not acc.startswith('sp|'):\n",
    "                    accs.add(acc.split()[0].split('|', maxsplit=2)[2])\n",
    "            except IndexError:\n",
    "                accs.add(acc.split()[0])\n",
    "    elif db_type == 'custom':\n",
    "        accs = {acc.strip() for acc in pd.read_csv(str(x_tandem_result_tsv), delimiter='\\t')['Protein'].tolist()}\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34952634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncbi_multiacc_to_accs_dict(ncbi_accs_from_file):\n",
    "    path_to_multiaccs = '/home/jules/Documents/databases/databases_tax2proteome/multispecies_acc'\n",
    "    # key = first acc in result tsv, value: all accs\n",
    "    multacc_reader = ReadAccTaxon(db_path, db_type, path_to_multiaccs)\n",
    "    multiacc2acc_dict = multacc_reader.read_multispecies_accs(ncbi_accs_from_file)\n",
    "    return multiacc2acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332bff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxa_from_acc2taxid_files(db_path, db_type, all_accs, taxa):\n",
    "    acc2tax_reader = ReadAccTaxon(db_path, db_type)\n",
    "    acc_2_taxon_dict = acc2tax_reader.read_acc2tax(all_accs, taxa)\n",
    "    return acc_2_taxon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94ea5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acc_from_file_2_taxa_set_dict(multiacc2acc_dict, acc_2_taxon_dict, ncbi_accs_from_file):\n",
    "    acc_in_tsv_2_taxa_set_dict = defaultdict(set)\n",
    "    for acc in ncbi_accs_from_file:\n",
    "        if acc in multiacc2acc_dict.keys():\n",
    "            for m_acc in multiacc2acc_dict[acc]:\n",
    "                try:\n",
    "                    acc_in_tsv_2_taxa_set_dict[acc].add(int(acc_2_taxon_dict[m_acc]))\n",
    "                    del acc_2_taxon_dict[m_acc]\n",
    "                # acc not in acc_2_taxon_dict, because taxon was not in taxa_all_level\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        else:\n",
    "            acc_in_tsv_2_taxa_set_dict[acc].add(int(acc_2_taxon_dict[acc]))\n",
    "            del acc_2_taxon_dict[acc]\n",
    "    return acc_in_tsv_2_taxa_set_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a69693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSM_FDR:\n",
    "    def __init__(self, path_to_file, path_to_crap, decoy_tag):\n",
    "        \"\"\"\n",
    "        :param path_to_file: path to xtandem output .xml converted to tsv\n",
    "        one line one header with all accessions seperated by \\t\n",
    "        \"\"\"\n",
    "        self.path_to_file = path_to_file\n",
    "        print('new version 2')\n",
    "        self.crap_acc_set = self.get_all_crap_accs(path_to_crap)\n",
    "        self.decoy_tag = decoy_tag\n",
    "        self.decoy_list = ['DECOY', 'CRAP', 'DECOY/CRAP', 0, '0']\n",
    "        self.sorted_xtandem_df = None\n",
    "        self.fdr_pos = 0\n",
    "        self.number_psms = 0\n",
    "        self.decoys = 0\n",
    "\n",
    "    def flatten_set(self, s):\n",
    "        return {item for sublist in s for item in sublist}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_crap_accs(path_to_crap):\n",
    "        crap_acc_set = set()\n",
    "        with open(str(path_to_crap)) as crap:\n",
    "            for line in crap.readlines():\n",
    "                if line.startswith('>'):\n",
    "                    crap_acc_set.add(line[1:].strip())\n",
    "        return crap_acc_set\n",
    "\n",
    "    def add_level_specific_taxid_column(self, taxon_graph, level):\n",
    "        self.sorted_xtandem_df[f'taxID_{level}'] = self.sorted_xtandem_df[f'taxID'].apply(lambda taxid:\n",
    "                                                taxon_graph.find_level_up(taxid, level)\n",
    "                                                if taxid not in self.decoy_list else taxid)\n",
    "        return self.sorted_xtandem_df\n",
    "\n",
    "    def group_df(self, level):\n",
    "        reduced_df = self.sorted_xtandem_df.groupby([\"#SpecFile\", 'Title', 'Peptide', 'Hyperscore'], as_index=False).agg(\n",
    "            {'Protein': lambda acc: set(acc), 'EValue': lambda x: set(list(x)), 'decoy': lambda x: set(x),\n",
    "             'taxID': lambda taxid: set(taxid), f'taxID_{level}': lambda x: set(x)})\n",
    "        return reduced_df\n",
    "\n",
    "    def get_taxid_of_prot_acc(self, protein_acc, acc2tax_dict, db_type):\n",
    "        if db_type=='ncbi':\n",
    "            if protein_acc in self.crap_acc_set:\n",
    "                return {'CRAP'}\n",
    "            if self.decoy_tag in protein_acc:\n",
    "                return {'DECOY'}\n",
    "            try:\n",
    "                # get taxa set\n",
    "                return acc2tax_dict[protein_acc]\n",
    "            except KeyError:\n",
    "                print(protein_acc)\n",
    "                return {'DECOY/CRAP'}\n",
    "        else:\n",
    "            if protein_acc in self.crap_acc_set:\n",
    "                return 'CRAP'\n",
    "            if self.decoy_tag in protein_acc:\n",
    "                return 'DECOY'\n",
    "            try:\n",
    "                if db_type == 'uniprot' or db_type == 'swissprot':\n",
    "                    return int(acc2tax_dict[protein_acc.split('|')[1]])\n",
    "                elif db_type == 'custom':\n",
    "                    return int(acc2tax_dict[protein_acc.strip()])\n",
    "            except KeyError:\n",
    "                print(protein_acc)\n",
    "                return 'DECOY/CRAP'\n",
    "\n",
    "    def create_PSM_dataframe_for_uniprot_accs(self, acc2tax_dict, taxon_graph, level):\n",
    "        self.sorted_xtandem_df['Protein'] = self.sorted_xtandem_df['Protein'].apply(lambda protein_acc: protein_acc.split()[0])\n",
    "        self.sorted_xtandem_df['taxID'] = self.sorted_xtandem_df['Protein'].apply(lambda protein_acc:\n",
    "                                                                self.get_taxid_of_prot_acc(protein_acc, acc2tax_dict, 'uniprot'))\n",
    "\n",
    "        self.sorted_xtandem_df = self.add_level_specific_taxid_column(taxon_graph, level)\n",
    "        reduced_df = self.group_df(level)\n",
    "        return reduced_df\n",
    "\n",
    "    def get_first_acc(self, acc):\n",
    "        if not acc.startswith('generic'):\n",
    "            return acc\n",
    "        decoy = True if self.decoy_tag in acc else False\n",
    "        if decoy:\n",
    "            return acc.split('|', maxsplit=2)[2].split()[0] + '_' + self.decoy_tag\n",
    "        else:\n",
    "            return acc.split('|', maxsplit=2)[2].split()[0]\n",
    "\n",
    "    def create_PSM_dataframe_for_ncbi_accs(self, acc2tax_dict, taxon_graph, level):\n",
    "        \"\"\"\n",
    "        :param acc2tax_dict: for ncbi multiple accs per accs posiible, so multiple taxa posiible: {acc string: {set of taxa(int)}\n",
    "        :param taxon_graph: loaded TaxonGraph\n",
    "        :param level: 'species', 'genus', ...\n",
    "        :return: reduced_df same spectra ID and Hyperscore entries combined into one, all values into sets\n",
    "        \"\"\"\n",
    "        # Multiaccs in Protein column: 'PNW76085.1 BAB64417.1 BAB64413.1 XP_001693987.1'\n",
    "        print('get first accs in protein column')\n",
    "        self.sorted_xtandem_df['Protein'] = \\\n",
    "            self.sorted_xtandem_df['Protein'].apply(lambda acc: self.get_first_acc(acc))\n",
    "        print('get taxIDs of protein accs')\n",
    "        self.sorted_xtandem_df['taxID'] = self.sorted_xtandem_df['Protein'].apply(lambda protein_acc:\n",
    "                                                                                  self.get_taxid_of_prot_acc(protein_acc, acc2tax_dict, 'ncbi'))\n",
    "        print(self.sorted_xtandem_df['taxID'][0:5])\n",
    "        # 'Protein': generic|AC:4260012|KKY45073.1 WP_046834534.1, generic|AC:6172351|WP_086660554.1 ...\n",
    "        # NCBI: 'taxID': {83334}, {562}, takes lot of time\n",
    "        print('get taxIDs of specified level')\n",
    "        self.sorted_xtandem_df[f'taxID_{level}'] = \\\n",
    "            self.sorted_xtandem_df['taxID'].apply(lambda taxID_set: {taxon_graph.find_level_up(int(taxID), level)\n",
    "                                                                     if taxID not in self.decoy_list else taxID\n",
    "                                                                     for taxID in taxID_set})\n",
    "        print('reducing df')\n",
    "        reduced_df = self.sorted_xtandem_df.groupby([\"#SpecFile\", 'Title', 'Peptide', 'Hyperscore'], as_index=False).agg(\n",
    "            {'Protein': lambda acc: set(acc), 'EValue': lambda x: set(list(x)), 'decoy': lambda decoy: set(decoy),\n",
    "             'taxID': lambda taxid_sets: self.flatten_set(taxid_sets),\n",
    "             f'taxID_{level}': lambda taxid_sets: self.flatten_set(taxid_sets)})\n",
    "        return reduced_df\n",
    "\n",
    "    def create_PSM_dataframe_for_custom_accs(self, acc2tax_dict, taxon_graph, level):\n",
    "        self.sorted_xtandem_df['taxID'] = self.sorted_xtandem_df['Protein'].apply(\n",
    "            lambda acc: self.get_taxid_of_prot_acc(acc, acc2tax_dict))\n",
    "        self.sorted_xtandem_df = self.add_level_specific_taxid_column(taxon_graph, level)\n",
    "        reduced_df = self.group_df(level)\n",
    "        return reduced_df\n",
    "\n",
    "    def create_PSM_dataframe(self, db_type, level, taxon_graph, acc2tax_dict):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: Hyperscore sorted dataframe, Protein with highest Hyperscore first, Title = spectra_file_spectraID\n",
    "        \"\"\"\n",
    "        xtandem_df = pd.read_csv(str(self.path_to_file), delimiter='\\t')\n",
    "        xtandem_df['Protein'] = xtandem_df['Protein'].apply(lambda acc: acc.strip())\n",
    "        # change spectra Title\n",
    "        xtandem_df['Title'] = xtandem_df['Title'].apply(lambda row: row.split(' File')[0])\n",
    "        print('Sorting panda dataframe by hyperscore. Find taxa, and level taxa')\n",
    "        self.sorted_xtandem_df = xtandem_df.sort_values(by=['Hyperscore', 'Title'], ascending=False).reset_index(drop=True)\n",
    "        self.sorted_xtandem_df['decoy'] = self.sorted_xtandem_df.apply(lambda row: True if self.decoy_tag in row['Protein'] else False, axis=1)\n",
    "        print('create dataframe with level taxa')\n",
    "        if db_type == 'uniprot' or db_type == 'swissprot':\n",
    "            reduced_df = self.create_PSM_dataframe_for_uniprot_accs(acc2tax_dict, taxon_graph, level)\n",
    "        elif db_type == 'ncbi':\n",
    "            reduced_df = self.create_PSM_dataframe_for_ncbi_accs(acc2tax_dict, taxon_graph, level)\n",
    "        elif db_type == 'custom':\n",
    "            reduced_df = self.create_PSM_dataframe_for_custom_accs(acc2tax_dict, taxon_graph, level)\n",
    "\n",
    "        # sort by Hyperscore\n",
    "        reduced_df = reduced_df.sort_values(by=['Hyperscore', 'Title'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "        print(f'entries in sorted df: {len(self.sorted_xtandem_df)} '\n",
    "              f'decoys in sorted df: {len(self.sorted_xtandem_df[self.sorted_xtandem_df.decoy==True])} '\n",
    "              f'hits in sorted df: {len(self.sorted_xtandem_df[self.sorted_xtandem_df.decoy==False])}')\n",
    "        print(f'entries in reduced_df: {len(reduced_df)} '\n",
    "              f'decoys in reduced_df: {len(reduced_df[reduced_df.decoy=={True}])} '\n",
    "              f'hits in reduced_df: {len(reduced_df[reduced_df.decoy=={False}])} '\n",
    "              f'mixed in reduced_df: {len(reduced_df[reduced_df.decoy=={False, True}])}')\n",
    "        return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262a9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncbi_acc2taxon_dict(ncbi_accs_from_file, db_path, db_type, taxa):\n",
    "    \"\"\"\n",
    "    :param x_tandem_result_tsv:\n",
    "    :param db_path:\n",
    "    :param db_type:\n",
    "    :param taxa: list of all relevant taxa, all taxa in taxon graph below specified level\n",
    "    :return: acc_in_tsv_2_taxa_set_dict dict {acc string: {set of taxa(int)}\n",
    "    \"\"\"\n",
    "    # Protein: generic|AC:2905764_REVERSED|VVN60222.1-REVERSED, generic|AC:4682148|PMZ83684.1 PMZ91489.1 WP_054593944.1 PMZ34596.1 ALI00421.1 PMZ37242.1...\n",
    "    # generic|AC:3582431_REVERSED|WP_032249104.1 KDU12656.1 KDT75424.1-REVERSED\n",
    "    \n",
    "    \n",
    "    # for ncbi find all multispecies accessions with file ncbi acc\n",
    "    multiacc2acc_dict = get_ncbi_multiacc_to_accs_dict(ncbi_accs_from_file)\n",
    "    # key = Multiacc or accs assigned to multiaccs\n",
    "    all_multiaccs = set(flatten_list(multiacc2acc_dict.values()))\n",
    "    all_accs = all_multiaccs.union(ncbi_accs_from_file)   \n",
    "    acc_2_taxon_dict = get_taxa_from_acc2taxid_files(db_path, db_type, all_accs, taxa)\n",
    "    acc_in_tsv_2_taxa_set_dict = create_acc_from_file_2_taxa_set_dict(all_accs, multiacc2acc_dict, acc_2_taxon_dict)\n",
    "    \n",
    "    return acc_in_tsv_2_taxa_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f33cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_with_decoy_or_nan_in_reference_and_result(Ref_taxID_level_column, taxID_level_column):\n",
    "        true_false_list = []\n",
    "        for ref_taxID_set, taxID_set in zip(Ref_taxID_level_column, taxID_level_column):\n",
    "            if ref_taxID_set == {'DECOY'} and taxID_set == {'DECOY'}:\n",
    "                true_false_list.append(False)\n",
    "            elif (ref_taxID_set == {'DECOY'} and type(taxID_set) != set) or (type(ref_taxID_set) != set and taxID_set == {'DECOY'}):\n",
    "                true_false_list.append(False)\n",
    "            else:\n",
    "                true_false_list.append(True)\n",
    "        return true_false_list\n",
    "    \n",
    "def get_all_spectra_IDs(ident_file):\n",
    "    all_spec_IDs = set()\n",
    "    with open(ident_file, 'r') as ident_file:\n",
    "        for line in ident_file:\n",
    "            if line.startswith('TITLE'):\n",
    "                all_spec_IDs.add(line.split()[0].split('TITLE=')[1])\n",
    "    return all_spec_IDs\n",
    "\n",
    "def rename_reference_df_columns(reference_df):\n",
    "    reference_df = reference_df[['Title', 'Peptide', 'Hyperscore', 'Protein', 'decoy', 'taxID', 'taxID_species']]\n",
    "    reference_df.columns = ['Title', 'Ref_Peptide', 'Ref_Hyperscore', 'Ref_ProteinAcc', 'Ref_decoy', 'Ref_taxID_DB', 'Ref_taxID_species']\n",
    "    return reference_df\n",
    "\n",
    "def get_nan_or_decoy(ref_taxID_column, taxID_column):\n",
    "    true_false_list = []\n",
    "    for ref_taxID_set, taxID_set in zip(ref_taxID_column, taxID_column):\n",
    "        if type(ref_taxID_set) != set and type(taxID_set) != set:\n",
    "            true_false_list.append(True)\n",
    "        elif (ref_taxID_set == {'DECOY'} and type(taxID_set) != set) or (type(ref_taxID_set) != set and taxID_set == {'DECOY'}):\n",
    "            true_false_list.append(True)\n",
    "        else:\n",
    "            true_false_list.append(False)\n",
    "    return true_false_list\n",
    "\n",
    "def get_all_unidentified_spectra_in_reference_and_result(result_df, reduced_df):\n",
    "    spectra_file = '/home/jules/Documents/Tax2Proteome/benchmarking/spectra/Run1_U1_2000ng.mgf'\n",
    "    all_spectra_set = get_all_spectra_IDs(spectra_file)\n",
    "    df_with_all_spectra_and_reference_and_results =  HelperMethod.create_df_with_all_spectra_and_dfs_to_compare(\n",
    "            all_spectra_set, result_df, 'Title', reduced_df, 'Title')\n",
    "    df_with_all_unidentified_spectra = df_with_all_spectra_and_reference_and_results[get_nan_or_decoy(df_with_all_spectra_and_reference_and_results.Ref_taxID_DB, df_with_all_spectra_and_reference_and_results.taxID)]\n",
    "    return df_with_all_unidentified_spectra\n",
    "\n",
    "def check_for_TP(taxid_set, taxid_ref_set):\n",
    "    # TP: if all taxa from ref set contained in result set\n",
    "    # ignore Decoy crap entries from result_reduced (not contained in reference)\n",
    "    decoy_set = {'DECOY', 'DECOY/CRAP', 0}\n",
    "    taxid_set = taxid_set.difference(decoy_set)\n",
    "    if len(taxid_set) == 0 and pd.isna(taxid_ref_set): #only decoy entries\n",
    "        return False\n",
    "    return taxid_ref_set.issubset(taxid_ref_set)\n",
    "\n",
    "def check_for_FP(taxid_set, taxid_ref_set):\n",
    "    # FP: if all taxa from result_df not in reference_set\n",
    "    if len(taxid_set.intersection(taxid_ref_set)) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def compare_tax_sets(taxid_set, taxid_ref_set, is_FP):\n",
    "    if not pd.isna(taxid_set) and not pd.isna(taxid_ref_set):\n",
    "        if is_FP:\n",
    "            return check_for_FP(taxid_set, taxid_ref_set)\n",
    "        else:\n",
    "            return check_for_TP(taxid_set, taxid_ref_set)\n",
    "    return False\n",
    "\n",
    "def check_taxid_in_reference(taxid_level_column, taxid_level_ref_column, is_FP):\n",
    "        true_false_list = []\n",
    "        for taxid_set, taxid_ref_set in zip(taxid_level_column, taxid_level_ref_column):\n",
    "            true_false_list.append(compare_tax_sets(taxid_set, taxid_ref_set, is_FP))\n",
    "        return true_false_list\n",
    "\n",
    "def get_true_positive_and_true_negative(df_with_all_unidentified_spectra, df_with_all_reference_spectra_and_merged_results):\n",
    "        pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "        level = 'species'\n",
    "        df_taxid = df_with_all_reference_spectra_and_merged_results[['Title','taxID','Ref_taxID_DB']]\n",
    "        df_taxid_level = df_with_all_reference_spectra_and_merged_results[['Title', f'taxID_{level}',f'Ref_taxID_{level}']]\n",
    "        # both nan or one nan one DECOY\n",
    "        df_TN = df_with_all_unidentified_spectra\n",
    "       # print(df_TN.head())\n",
    "        TN=len(set(df_TN.SpectraID.tolist()))\n",
    "        \n",
    "        df_TP = df_taxid_level[check_taxid_in_reference(df_taxid_level[f'taxID_{level}'].tolist(), df_taxid_level[f'Ref_taxID_{level}'].tolist(), is_FP=False)]\n",
    "        TP = len(set(df_TP.Title.tolist()))\n",
    "        # FN: not identified in result, but identified in referernce\n",
    "        df_FN = df_taxid[(df_taxid.taxID.isna() ) & df_taxid.Ref_taxID_DB.notna()]\n",
    "        df_FN = df_FN[df_FN.Ref_taxID_DB != {'DECOY/CRAP'}]\n",
    "        FN=len(set(df_FN.Title.tolist()))\n",
    "\n",
    "\n",
    "        df_FP = df_taxid_level[check_taxid_in_reference(df_taxid_level[f'taxID_{level}'].tolist(), df_taxid_level[f'Ref_taxID_{level}'].tolist(), is_FP=True)]\n",
    "        FP = len(set(df_FP.Title.tolist()))\n",
    "        df_s = df_with_all_reference_spectra_and_merged_results[df_with_all_reference_spectra_and_merged_results.taxID != {'DECOY/CRAP'}]\n",
    "        print(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}, number of all spectra without decoy/crap spectra: \"\n",
    "              f\"{len(set(df_s.Title))}, number of TP+TN+FP+FN: {TP+FN+FP+TN}\")\n",
    "        return TP, FP, TN, FN\n",
    "\n",
    "def get_row_with_reference_nan_or_decoy_and_result_identified(ref_taxID_column, taxID_column):\n",
    "    true_false_list = []\n",
    "    for ref_taxID_set, taxID_set in zip(ref_taxID_column, taxID_column):\n",
    "        if type(ref_taxID_set) != set and type(taxID_set) == set:\n",
    "            if taxID_set not in [{'DECOY'},  {'DECOY/CRAP'}, {0}]:\n",
    "                true_false_list.append(True)\n",
    "            else:\n",
    "                true_false_list.append(False)\n",
    "        elif ref_taxID_set in [{'DECOY'},  {'DECOY/CRAP'}, {0}] and taxID_set not in [{'DECOY'},  {'DECOY/CRAP'}, {0}]:\n",
    "            true_false_list.append(True)\n",
    "        else:\n",
    "            true_false_list.append(False)\n",
    "    return true_false_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "015d75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from create_PSM_df import PSM_FDR\n",
    "from ReadAccTaxon import ReadAccTaxon\n",
    "from collections import defaultdict\n",
    "from handling_acc_files import HelperMethod\n",
    "\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def read_crap(file):\n",
    "    crap = set()\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if '|' in line:\n",
    "                    fields = [item for item in line.split('|')]\n",
    "                    crap.add(fields[1])\n",
    "                else:\n",
    "                    crap.add(line[1:].strip())\n",
    "    return crap\n",
    "\n",
    "def get_accs_from_df(x_tandem_result_tsv, db_type, decoy_tag):\n",
    "    acc_from_tsv = pd.read_csv(str(x_tandem_result_tsv), delimiter='\\t')['Protein'].tolist()\n",
    "    accs = get_accs_from_accs(acc_from_tsv, db_type, decoy_tag)\n",
    "    return accs\n",
    "\n",
    "def get_accs_from_accs(accs, db_type, decoy_tag):\n",
    "    processed_accs = set()\n",
    "    for acc in accs:\n",
    "        if db_type == 'uniprot' or db_type == 'swissprot':\n",
    "            try:\n",
    "                if decoy_tag not in acc:\n",
    "                    processed_accs.add(acc.split()[0].split('|')[1])\n",
    "            # e.g. CRAP: KKA1_ECOLX\n",
    "            except IndexError:\n",
    "                processed_accs.add(acc.split()[0])\n",
    "        elif db_type == 'ncbi':\n",
    "            # ncbi: 'generic|AC:5988671|WP_080217951.1', 'generic|AC:2500558_REVERSED|EQV03804.1 ERA83742.1 WP_021536075.1-REVERSED'\n",
    "            # problem: generic|AC:373747|pir||D85980 WP_000133047.1 AAG58304.1 -> maxsplit=2\n",
    "            # CRAP accs starts with 'sp|' or Index Error\n",
    "            try:\n",
    "                # CRAP accs = sp|acc\n",
    "                if decoy_tag not in acc and not acc.startswith('sp|'):\n",
    "                    processed_accs.add(acc.split()[0].split('|', maxsplit=2)[2])\n",
    "            except IndexError:\n",
    "                accs.add(acc.split()[0])\n",
    "        elif db_type == 'custom':\n",
    "            if decoy_tag not in acc:\n",
    "                processed_accs.add(acc.strip())\n",
    "    return processed_accs\n",
    "\n",
    "\n",
    "def get_acc2taxon_dict(db_path, db_type, accs=None):\n",
    "    acc2tax_reader=ReadAccTaxon(db_path, db_type)\n",
    "    if db_type == 'custom':\n",
    "        acc_2_taxon_dict = acc2tax_reader.get_acc2taxonID_dict(db_path/'acc2tax_custom')\n",
    "    elif db_type == 'uniprot' or db_type == 'swissprot':\n",
    "        acc_2_taxon_dict = acc2tax_reader.read_acc2tax(accs)\n",
    "    return acc_2_taxon_dict\n",
    "\n",
    "\n",
    "def get_multispecies_accs(path_to_multispecies_acc, accs):\n",
    "    \"\"\"\n",
    "    :param path_to_multispecies_acc: path to multispecies file\n",
    "    :param accs: set of ncbi accs\n",
    "    :return: multi_acc to list of accs dict\n",
    "        \"\"\"\n",
    "    multiacc2accs_dict={}\n",
    "    with open(path_to_multispecies_acc, 'r') as input:\n",
    "        for line in input:\n",
    "            fields = line.split()\n",
    "            l = [acc in accs for acc in fields]\n",
    "            if any(l):\n",
    "                pos_of_acc = [i for i, x in enumerate(l) if x][0]\n",
    "                multiacc2accs_dict[fields[pos_of_acc]] = fields[0:]\n",
    "    return multiacc2accs_dict\n",
    "\n",
    "\n",
    "def get_tsv_multspecies_acc_to_taxa_dict(multiacc2acc_dict, acc_2_taxon_dict):\n",
    "    tsv_multi_acc_to_taxon_dict = defaultdict(set)\n",
    "    for acc, acc_list in multiacc2acc_dict.items():\n",
    "        for multi_acc in acc_list:\n",
    "            if multi_acc in acc_2_taxon_dict.keys():\n",
    "                tsv_multi_acc_to_taxon_dict[multi_acc].add(acc_2_taxon_dict[multi_acc])\n",
    "    return tsv_multi_acc_to_taxon_dict\n",
    "\n",
    "\n",
    "def remove_accs_with_unsupported_taxa(multi_acc_2_taxon_dict, taxa):\n",
    "    accs_with_not_matching_taxa = []\n",
    "    for acc, taxon in multi_acc_2_taxon_dict.items():\n",
    "        if int(taxon) not in taxa:\n",
    "            accs_with_not_matching_taxa.append(acc)\n",
    "    for acc in accs_with_not_matching_taxa:\n",
    "        del multi_acc_2_taxon_dict[acc]\n",
    "    return multi_acc_2_taxon_dict\n",
    "\n",
    "\n",
    "def get_ncbi_multiacc_to_accs_dict(ncbi_accs_from_file, db_path, db_type):\n",
    "    path_to_multiaccs = '/home/jules/Documents/databases/databases_tax2proteome/multispecies_acc'\n",
    "    # key = first acc in result tsv, value: all accs\n",
    "    multacc_reader = ReadAccTaxon(db_path, db_type, path_to_multiaccs)\n",
    "    multiacc2acc_dict = multacc_reader.read_multispecies_accs(ncbi_accs_from_file)\n",
    "    return multiacc2acc_dict\n",
    "\n",
    "\n",
    "def get_taxa_from_acc2taxid_files(db_path, db_type, all_accs, taxa):\n",
    "    acc2tax_reader = ReadAccTaxon(db_path, db_type)\n",
    "    acc_2_taxon_dict = acc2tax_reader.read_acc2tax(all_accs, taxa)\n",
    "    return acc_2_taxon_dict\n",
    "\n",
    "\n",
    "def create_acc_from_file_2_taxa_set_dict(ncbi_accs_from_file, multiacc2acc_dict, acc_2_taxon_dict):\n",
    "    acc_in_tsv_2_taxa_set_dict = defaultdict(set)\n",
    "    for acc in ncbi_accs_from_file:\n",
    "        if acc in multiacc2acc_dict.keys():\n",
    "            for m_acc in multiacc2acc_dict[acc]:\n",
    "                try:\n",
    "                    acc_in_tsv_2_taxa_set_dict[acc].add(int(acc_2_taxon_dict[m_acc]))\n",
    "                    #del acc_2_taxon_dict[m_acc]\n",
    "                # acc not in acc_2_taxon_dict, because taxon was not in taxa_all_level\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        else:\n",
    "            acc_in_tsv_2_taxa_set_dict[acc].add(int(acc_2_taxon_dict[acc]))\n",
    "            #del acc_2_taxon_dict[acc]\n",
    "    return acc_in_tsv_2_taxa_set_dict\n",
    "\n",
    "\n",
    "def get_ncbi_acc2taxon_dict(ncbi_accs_from_file, db_path, db_type, taxa):\n",
    "    \"\"\"\n",
    "    :param x_tandem_result_tsv:\n",
    "    :param db_path:\n",
    "    :param db_type:\n",
    "    :param taxa: list of all relevant taxa, all taxa in taxon graph below specified level\n",
    "    :return: acc_in_tsv_2_taxa_set_dict dict {acc string: {set of taxa(int)}\n",
    "    \"\"\"\n",
    "    # for ncbi find all multispecies accessions with file ncbi acc\n",
    "    # key = Multiacc or accs assigned to multiaccs\n",
    "    multiacc2acc_dict = get_ncbi_multiacc_to_accs_dict(ncbi_accs_from_file, db_path, db_type)\n",
    "    all_accs = set(flatten_list(multiacc2acc_dict.values())).union(ncbi_accs_from_file)\n",
    "    # acc_2_taxon_dict contains 0 values for some accessions (are in prot.acc...)\n",
    "    acc_2_taxon_dict = get_taxa_from_acc2taxid_files(db_path, db_type, all_accs, taxa)\n",
    "    all_accs.clear()\n",
    "    # acc_in_tsv_2_taxa_set_dict contains in taxa_set items like '345' and '0'\n",
    "    acc_in_tsv_2_taxa_set_dict = create_acc_from_file_2_taxa_set_dict(ncbi_accs_from_file, multiacc2acc_dict, acc_2_taxon_dict)\n",
    "    multiacc2acc_dict.clear()\n",
    "    return acc_in_tsv_2_taxa_set_dict\n",
    "\n",
    "def create_reduced_df(db_path, path_to_x_tandem_result_tsv, db_type, level, taxonset, add_db=None, taxon=None):\n",
    "    Kleiner_taxIDs = [262724, 882, 176299, 228410, 44577, 926571, 323848, 12022, 1283336, 10754, 101570, 224308, 1041145,\n",
    "                      1004788, 266265, 266264, 99287, 1302247, 1149133, 3055, 93061, 1977402, 1114970, 511145,\n",
    "                      536, 1407502, 1294143,1619948]\n",
    "    Tanca_taxIDs = [747, 5535, 655183, 1579, 1255, 4932, 1465, 1351, 562]\n",
    "    if taxonset == 'kleiner':\n",
    "        taxonIDs = Kleiner_taxIDs\n",
    "        levels = ['subspecies', 'species', 'genus', 'family', 'order', 'superkingdom']\n",
    "    elif taxonset == 'tanca':\n",
    "        taxonIDs = Tanca_taxIDs\n",
    "        levels = ['species', 'genus', 'family', 'order', 'superkingdom']\n",
    "    if taxon:\n",
    "        taxonIDs.extend([taxID for taxonlist in options.taxon for taxID in taxonlist])\n",
    "\n",
    "    path_to_x_tandem_result_tsv = Path(path_to_x_tandem_result_tsv)\n",
    "    path_to_crap = Path(options.crap)\n",
    "    decoy_tag = options.decoy\n",
    "    taxon_graph = HelperMethod.load_taxa_graph(Path(options.tax_graph))\n",
    "\n",
    "    # acc_2_taxon_dict for identification file identified accs\n",
    "    if db_type == 'custom':\n",
    "        accs_in_tsv = set()\n",
    "    else:\n",
    "        accs_in_tsv = get_accs_from_df(path_to_x_tandem_result_tsv, db_type, decoy_tag)\n",
    "\n",
    "\n",
    "    if db_type == 'ncbi':\n",
    "        all_taxa_of_level_set = set(flatten_list(taxon_graph.get_all_taxids(taxonIDs, level)))\n",
    "        # acc_in_tsv_2_taxa_set_dict\n",
    "        acc_2_taxon_dict = get_ncbi_acc2taxon_dict(accs_in_tsv, db_path, db_type, all_taxa_of_level_set)\n",
    "    else:\n",
    "        acc_2_taxon_dict = get_acc2taxon_dict(db_path, db_type, accs_in_tsv)\n",
    "    accs_in_tsv.clear()\n",
    "\n",
    "    if add_db:\n",
    "        # accs= full accs without split, if custom: get dict of all accs in db\n",
    "        accs = get_accs_from_df(path_to_x_tandem_result_tsv, 'custom', decoy_tag)\n",
    "        add_accs = {acc for acc in accs if acc not in acc_2_taxon_dict.keys()}\n",
    "        processed_add_acc = get_accs_from_accs(add_accs, options.add_db, decoy_tag)\n",
    "        processed_add_acc_2_taxon_dict = get_acc2taxon_dict(db_path, options.add_db, processed_add_acc)\n",
    "        add_acc_2_taxon_dict = {}\n",
    "        for processed_acc, taxon in processed_add_acc_2_taxon_dict.items():\n",
    "            for acc in add_accs:\n",
    "                if processed_acc in acc:\n",
    "                    add_acc_2_taxon_dict[acc] = taxon\n",
    "        acc_2_taxon_dict.update(add_acc_2_taxon_dict)\n",
    "    for k, v in acc_2_taxon_dict.items():\n",
    "        if v == 0 or v=='0':\n",
    "            print(k,v)\n",
    "\n",
    "    psm = PSM_FDR(path_to_x_tandem_result_tsv, path_to_crap, decoy_tag)\n",
    "    reduced_df = psm.create_PSM_dataframe(db_type, level, taxon_graph, acc_2_taxon_dict)\n",
    "    print(f\"writing data frame to {path_to_x_tandem_result_tsv.parent.joinpath(path_to_x_tandem_result_tsv.stem + '_reduced.tsv')}... \")\n",
    "    reduced_df.to_csv(str(path_to_x_tandem_result_tsv.parent.joinpath(path_to_x_tandem_result_tsv.stem + '_reduced.tsv')), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fd5dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files and paths\n",
    "path_to_crap ='/home/jules/Documents/Tax2Proteome/benchmarking/Kleiner_ref_db/crap.fasta'\n",
    "db_path = Path('/home/jules/Documents/databases/databases_tax2proteome/')\n",
    "input_kleiner_reference_aradiopsis = '/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_kleiner_reference_aradiopsis/Run1_U1_2000ng_kleiner_aradiopsis.t.xml.tsv.tsv'\n",
    "path_to_kleiner_acc2tax = '/home/jules/Documents/Tax2Proteome/benchmarking/Kleiner_ref_db'\n",
    "crap_file = '/home/jules/Documents/Tax2Proteome/benchmarking/Kleiner_ref_db/crap.fasta'\n",
    "path_to_taxdump = '/home/jules/Documents/databases/databases_tax2proteome/taxdump.tar.gz'\n",
    "path_to_ncbi_species_x_tandem_result_tsv = Path('/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_species.t.xml.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "388c4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxa\n",
    "Kleiner_taxIDs = [262724, 882, 176299, 228410, 44577, 926571, 323848, 12022, 1283336, 10754, 101570, 224308, 216596,\n",
    "                      1004788, 266265, 266264, 99287, 1294143, 1149133, 3055, 1280, 1977402, 294, 83333,\n",
    "                      536, 1407502]\n",
    "Tanca_taxIDs = [747, 5535, 655183, 1579, 1255, 4932, 1465, 1351, 562]\n",
    "decoy_tag = 'REVERSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCBI\n",
    "db_type = 'ncbi'\n",
    "level = 'species'\n",
    "%run create_reduced_df_with_taxa.py -i /home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_subspecies.t.xml.tsv -p /home/jules/Documents/databases/databases_tax2proteome/ -c /home/jules/Documents/Tax2Proteome/benchmarking/Kleiner_ref_db/crap.fasta -g /home/jules/Documents/databases/databases_tax2proteome/taxdump.tar.gz -l subspecies -d ncbi -s kleiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c7a2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCBI step by step\n",
    "path_to_x_tandem_result_tsv=Path(\"/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_subspecies.t.xml.tsv\")\n",
    "accs = get_accs_from_df(path_to_x_tandem_result_tsv, \"ncbi\", decoy_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f67c4083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load taxon graph from harddrive.\n"
     ]
    }
   ],
   "source": [
    "taxon_graph = HelperMethod.load_taxa_graph(Path(path_to_taxdump))\n",
    "all_taxa_of_level_set = set(flatten_list(taxon_graph.get_all_taxids(Kleiner_taxIDs, 'species')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fb4de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading multiaccs database file with 8 threads.\n",
      "10% read.\n",
      "20% read.\n",
      "30% read.\n",
      "40% read.\n",
      "50% read.\n",
      "60% read.\n",
      "70% read.\n",
      "80% read.\n",
      "90% read.\n"
     ]
    }
   ],
   "source": [
    "# acc_2_taxon_dict = get_ncbi_acc2taxon_dict(accs, db_path, db_type, all_taxa_of_level_set)\n",
    "multiacc2acc_dict = get_ncbi_multiacc_to_accs_dict(accs, db_path, 'ncbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a986c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = set(flatten_list(multiacc2acc_dict.values())).union(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c3de51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading accession2prot database file with 8 threads.\n",
      "10% read.\n",
      "20% read.\n",
      "30% read.\n",
      "40% read.\n",
      "50% read.\n",
      "60% read.\n",
      "70% read.\n",
      "80% read.\n",
      "90% read.\n"
     ]
    }
   ],
   "source": [
    "# acc_2_taxon_dict contains 0 values for some accessions (are in prot.acc...) -> not without del in function\n",
    "acc_2_taxon_dict = get_taxa_from_acc2taxid_files(db_path, db_type, all_accs, all_taxa_of_level_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b50cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_accs.clear()\n",
    "# acc_in_tsv_2_taxa_set_dict contains in taxa_set items like '345' and '0'\n",
    "acc_in_tsv_2_taxa_set_dict = create_acc_from_file_2_taxa_set_dict(accs, multiacc2acc_dict, acc_2_taxon_dict)\n",
    "#multiacc2acc_dict.clear()\n",
    "acc_2_taxon_dict = acc_in_tsv_2_taxa_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39742f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in acc_2_taxon_dict.items():\n",
    "        if v == 0 or v=='0':\n",
    "            print(k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e676079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting panda dataframe by hyperscore. Find taxa, and level taxa\n",
      "create dataframe with level taxa\n",
      "get first accs in protein column\n",
      "get taxIDs of protein accs\n",
      "get taxIDs of specified level\n",
      "reducing df\n",
      "entries in sorted df: 311814 decoys in sorted df: 93867 hits in sorted df: 217947\n",
      "entries in reduced_df: 131639 decoys in reduced_df: 40319 hits in reduced_df: 91232 mixed in reduced_df: 88\n"
     ]
    }
   ],
   "source": [
    "psm = PSM_FDR(path_to_x_tandem_result_tsv, path_to_crap, decoy_tag)\n",
    "reduced_df = psm.create_PSM_dataframe(db_type, level, taxon_graph, acc_2_taxon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43914556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data frame to /home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_subspecies.t.xml_reduced.tsv... \n"
     ]
    }
   ],
   "source": [
    "print(f\"writing data frame to {Path(path_to_x_tandem_result_tsv).parent.joinpath(Path(path_to_x_tandem_result_tsv).stem + '_reduced.tsv')}... \")\n",
    "reduced_df.to_csv(str(Path(path_to_x_tandem_result_tsv).parent.joinpath(Path(path_to_x_tandem_result_tsv).stem + '_reduced.tsv')), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52f4a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load taxon graph from harddrive.\n",
      "Start reading accession2prot database file with 8 threads.\n",
      "10% read.\n",
      "20% read.\n",
      "30% read.\n",
      "40% read.\n",
      "50% read.\n",
      "60% read.\n",
      "70% read.\n",
      "80% read.\n",
      "90% read.\n",
      "Sorting panda dataframe by hyperscore. Find taxa, and level taxa\n",
      "create dataframe with level taxa\n",
      "entries in sorted df: 201790 decoys in sorted df: 61298 hits in sorted df: 140492\n",
      "entries in reduced_df: 130684 decoys in reduced_df: 41153 hits in reduced_df: 89445 mixed in reduced_df: 86\n",
      "writing data frame to /home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/uniprot/x_tandem_tsv/Run1_U1_2000ng_uniprot_subspecies_nr.t.xml_reduced.tsv... \n"
     ]
    }
   ],
   "source": [
    "# Uniprot\n",
    "%run create_reduced_df_with_taxa.py -i /home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/uniprot/x_tandem_tsv/Run1_U1_2000ng_uniprot_subspecies_nr.t.xml.tsv -p /home/jules/Documents/databases/databases_tax2proteome/ -c /home/jules/Documents/Tax2Proteome/benchmarking/Kleiner_ref_db/crap.fasta -g /home/jules/Documents/databases/databases_tax2proteome/taxdump.tar.gz -l species -d uniprot -s kleiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = psm.create_PSM_dataframe(db_type, level, taxon_graph, acc_2_taxon_dict)\n",
    "print(f\"writing data frame to {path_to_ncbi_species_x_tandem_result_tsv.parent.joinpath(path_to_x_tandem_result_tsv.stem + '_reduced.tsv')}... \")\n",
    "reduced_df.to_csv(str(path_to_ncbi_species_x_tandem_result_tsv.parent.joinpath(path_to_ncbi_species_x_tandem_result_tsv.stem + '_reduced.tsv')), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa3a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run anaylze_identification_files.py -p /home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/uniprot/x_tandem_tsv/Run1_U1_2000ng_uniprot_subspecies_nr.t.xml.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ef987",
   "metadata": {},
   "outputs": [],
   "source": [
    " # NCBI\n",
    "db_path = Path('/home/jules/Documents/databases/databases_tax2proteome/')\n",
    "db_type = 'ncbi'\n",
    "# for reduced_df True\n",
    "is_decoy_column_set = True\n",
    "x_tandem_result_tsv = '/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_species.t.xml.tsv'\n",
    "decoy_tag = 'REVERSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_taxa_of_level_set = set(flatten_list(taxon_graph.get_all_taxids(taxonIDs, 'species')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aae727",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_2_taxon_dict = get_ncbi_acc2taxon_dict(x_tandem_result_tsv, db_path, db_type, all_taxa_of_level_set, decoy_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b99aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(acc_2_taxon_dict.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "psm = PSM_FDR(x_tandem_result_tsv)\n",
    "reduced_df = psm.create_PSM_dataframe(decoy_tag, db_type, 'species', taxon_graph, acc2tax_dict=acc_2_taxon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9730336",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_pos, number_psms, decoys = psm.determine_FDR_position(reduced_df, options.fdr, is_decoy_column_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tandem_result_tsv = '/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_genus.t.xml.tsv'\n",
    "all_taxa_of_level_set = set(flatten_list(taxon_graph.get_all_taxids(taxonIDs, 'genus')))\n",
    "acc_2_taxon_dict = get_ncbi_acc2taxon_dict(x_tandem_result_tsv, db_path, db_type, all_taxa_of_level_set, decoy_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psm = PSM_FDR(x_tandem_result_tsv)\n",
    "reduced_df_genus = psm.create_PSM_dataframe(decoy_tag, db_type, 'species', taxon_graph, acc2tax_dict=acc_2_taxon_dict)\n",
    "reduced_df_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = ReferenceWriter.read_csv_with_generic_function('/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_xtandem_analyzer_bachelor_thesis/ncbi_kleiner/x_tandem_tsv/Run1_U1_2000ng_ncbi_kleiner_species.t.xml_reduced.tsv',\n",
    "                                            ['Protein', 'decoy', 'taxID', f'taxID_species'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "psm = PSM_FDR(x_tandem_result_tsv)\n",
    "fdr_pos, number_psms, decoys = psm.determine_FDR_position(reduced_df, 0.05, is_decoy_column_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b50ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = ReferenceWriter.read_csv_with_generic_function('/home/jules/Documents/Tax2Proteome/benchmarking/results_searchgui_kleiner_db/Run1_U1_2000ng.t.xml_reduced.tsv',\n",
    "                                           ['Protein', 'decoy', 'taxID', f'taxID_species'])\n",
    "reference_df = reference_df[['Title', 'Peptide', 'Hyperscore', 'Protein', 'decoy', 'taxID', 'taxID_species']]\n",
    "fdr_pos1, number_psms1, decoys1 = psm.determine_FDR_position(reduced_df, 0.05, True)\n",
    "fdr_pos2, number_psms2, decoys2 = psm.determine_FDR_position(reference_df, 0.05, True)\n",
    "fdr_applied_reference_df = reference_df[0:fdr_pos2]\n",
    "fdr_applied_reduced_df =reduced_df[0:fdr_pos1]\n",
    "fdr_applied_reference_df.columns = ['Title', 'Ref_Peptide', 'Ref_Hyperscore', 'Ref_ProteinAcc', 'Ref_decoy', 'Ref_taxID_DB', 'Ref_taxID_species']\n",
    "print(fdr_applied_reference_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all_reference_spectra_and_merged_results = pd.merge(fdr_applied_reference_df, fdr_applied_reduced_df, how=\"left\",\n",
    "                                                                    left_on='Title', right_on='Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dce7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640cebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove DECOYs\n",
    "df_with_all_reference_spectra_and_merged_results = df_with_all_reference_spectra_and_merged_results[get_rows_with_decoy_or_nan_in_reference_and_result(df_with_all_reference_spectra_and_merged_results.Ref_taxID_species, df_with_all_reference_spectra_and_merged_results.taxID_species)]\n",
    "df_with_all_reference_spectra_and_merged_results.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reference_df.head())\n",
    "# reference_df = rename_reference_df_columns(reference_df)\n",
    "df_with_all_unidentified_spectra = get_all_unidentified_spectra_in_reference_and_result(reference_df, reduced_df)\n",
    "#print(df_with_all_unidentified_spectra.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4798d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_true_positive_and_true_negative(df_with_all_unidentified_spectra, df_with_all_reference_spectra_and_merged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_all_result_spectra_and_merged_reference_in_fdr = pd.merge(fdr_applied_reduced_df, fdr_applied_reference_df, how=\"left\",\n",
    "                                                                           on='Title')\n",
    "df_only_result = df_with_all_result_spectra_and_merged_reference_in_fdr[get_row_with_reference_nan_or_decoy(\n",
    "df_with_all_result_spectra_and_merged_reference_in_fdr.Ref_taxID_DB, df_with_all_result_spectra_and_merged_reference_in_fdr.taxID)]\n",
    "df_only_result.head(3)\n",
    "print(len(df_only_result))\n",
    "len(set(df_only_result.Title.tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
